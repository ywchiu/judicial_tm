{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 大/ 巨蛋/ 案/ 對/ 市府/ 同仁/ 下/ 封口/ 封口令/ 口令/ / / / 柯/ P/ 否/ 認\n",
      "Default Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 　/ 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"userdict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 a\n",
      "巨蛋 n\n",
      "案 ng\n",
      "對 p\n",
      "市府 n\n",
      "同仁 nr\n",
      "下 f\n",
      "封口令 n\n",
      "？ x\n",
      "　 x\n",
      "柯 nr\n",
      "P eng\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\")\n",
    "for w in words:\n",
    "    print(w.word, w.flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 　/ 柯P/ 否認\n",
      "大巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 　/ 柯P/ 否認\n"
     ]
    }
   ],
   "source": [
    "sentence = \"大巨蛋案對市府同仁下封口令？　柯P否認\"\n",
    "words = jieba.cut(sentence, cut_all=False)\n",
    "print(\"/ \".join(words))\n",
    "\n",
    "jieba.add_word('柯P',100, 'nr')\n",
    "jieba.add_word('大巨蛋',100, 'ns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大巨蛋 0 3\n",
      "案對 3 5\n",
      "市府 5 7\n",
      "同仁 7 9\n",
      "下 9 10\n",
      "封口令 10 13\n",
      "？ 13 14\n",
      "　 14 15\n",
      "柯P 15 17\n",
      "否認 17 19\n"
     ]
    }
   ],
   "source": [
    "words = jieba.tokenize(sentence)\n",
    "\n",
    "for tw in words:\n",
    "    print(tw[0], tw[1], tw[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "封口令\n",
      "柯P\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "tags = jieba.analyse.extract_tags(sentence, 1)\n",
    "print(\",\".join(tags))\n",
    "tags = jieba.analyse.extract_tags(sentence, 1, allowPOS = ['nr'])\n",
    "print(\",\".join(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歐洲/電商/組織/包裹/物流/集團/結合/根據/國的/更為/領域/市場/決電商/緊密/一來/16/障礙/可解/遞送/積極\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "sentence = \"根據歐洲電商組織(Ecommerce Europe) 最新消息得知，\\\n",
    "該集團積極讓歐洲的包裹遞送服務與電商領域有更緊密結合。如此一來，\\\n",
    "便可解決電商物流的障礙。此組織總共結合歐洲 16 國的電商協會，\\\n",
    "目標整合歐洲的包裹物流市場，讓商品跨境更為順暢\"\n",
    "tags = jieba.analyse.extract_tags(sentence)\n",
    "print('/'.join(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我\n",
      "我們\n",
      "們酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n"
     ]
    }
   ],
   "source": [
    "sentence='那我們酸民婉君也可以報名嗎'\n",
    "\n",
    "for i in range(0, len(sentence) - 2 + 1):\n",
    "    print(sentence[i:i+2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那我們\n",
      "我們酸\n",
      "們酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sentence) -3 + 1):\n",
    "    print(sentence[i:i+3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    word_dic = {}\n",
    "    for i in range(0, len(sentence) - n + 1):\n",
    "        if sentence[i:i+n] not in word_dic:\n",
    "            word_dic[sentence[i:i+n]] = 1\n",
    "        else:\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\n",
    "    return word_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'也可': 1,\n",
       " '以報': 1,\n",
       " '們酸': 1,\n",
       " '可以': 1,\n",
       " '名嗎': 1,\n",
       " '君也': 1,\n",
       " '報名': 1,\n",
       " '婉君': 1,\n",
       " '我們': 1,\n",
       " '民婉': 1,\n",
       " '那我': 1,\n",
       " '酸民': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們 1\n",
      "以報 1\n",
      "那我 1\n",
      "報名 1\n",
      "婉君 1\n",
      "名嗎 1\n",
      "民婉 1\n",
      "君也 1\n",
      "可以 1\n",
      "們酸 1\n",
      "酸民 1\n",
      "也可 1\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "dic = ngram(sentence, 2)\n",
    "words_freq = sorted(dic.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "for word in words_freq:\n",
    "    if word[1] >= 2:\n",
    "        print(word[0], word[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeKey(text, keyword):\n",
    "    textAry= text\n",
    "    for key in keyword:\n",
    "        textAry = ''.join(textAry.split(key.decode('utf-8')))\n",
    "    return textAry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最高法院及最高行政法院裁判資料於評決後一個月提供查詢\n",
      "其餘法院於書記官製作裁判正本\n",
      "刑事為收領原本後七日內\n",
      "民事為收領原本後十日內\n",
      "並交付送達當事人後\n",
      "二日內提供查詢\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "delimiter = \"，|。|、|\\(|\\)\"  \n",
    "text = '最高法院及最高行政法院裁判資料於評決後一個月提供查詢。其餘法院於書記官製作裁判正本(刑事為收領原本後七日內，民事為收領原本後十日內)並交付送達當事人後，二日內提供查詢。'\n",
    "for i in re.split(delimiter, text):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'將公布'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeKey(text, keyword):\n",
    "    textAry= text\n",
    "    for key in keyword:\n",
    "        textAry = ''.join(textAry.split(key))\n",
    "    return textAry\n",
    "\n",
    "a = '民進黨後天將公布'\n",
    "removeKey(a, ['民進黨', '後天'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords=[]        \n",
    "ret_terms={}\n",
    "words_freq  = []\n",
    "for term_length in range(4,1,-1):\n",
    "    word_dic = {}\n",
    "    for sentence in sentenceAry:\n",
    "        text_list = removeKey(sentence,keywords)        \n",
    "        ngram_words = ngram(text_list,term_length) \n",
    "        for word in ngram_words:\n",
    "            if word not in word_dic:\n",
    "                word_dic[word] = 1\n",
    "            else:\n",
    "                word_dic[word] += ngram_words[word]   \n",
    "    for word in word_dic:\n",
    "        if word_dic[word] >= 2:\n",
    "            keywords.append(word)            \n",
    "            ret_terms.update({word:word_dic[word]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'收領原本': 2, '日內': 3, '為收領原': 2, '領原': 2, '法院': 2, '事為': 2, '為收領': 2, '收領': 2, '為收': 2, '本後': 2, '查詢': 2, '供查': 2, '收領原': 2, '領原本後': 2, '提供查': 2, '事為收領': 2, '領原本': 2, '原本': 2, '供查詢': 2, '原本後': 2, '事為收': 2, '提供': 2, '提供查詢': 2, '裁判': 2}\n"
     ]
    }
   ],
   "source": [
    "print(ret_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.270310072072\n",
      "0.0\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "\n",
    "print(tfidf(\"a\", a, D))\n",
    "print(tfidf(\"b\", abb, D))\n",
    "print(tfidf(\"a\", abc, D))\n",
    "print(tfidf(\"b\", abc, D))\n",
    "print(tfidf(\"c\", abc, D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "ary = ['【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理',\n",
    "'留洪智坤 柯：殘障求職不易',\n",
    "'人事處議處洪智坤　柯P：不清楚議處結果']\n",
    "corpus = []\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易\n",
      "人事\n",
      "今處理\n",
      "公文\n",
      "更新\n",
      "柯p\n",
      "案還\n",
      "殘障\n",
      "求職\n",
      "洩漏\n",
      "洪智坤\n",
      "清楚\n",
      "留洪智坤\n",
      "看到\n",
      "結果\n",
      "處議\n",
      "議處\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n",
    "for w in word:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.31377734  0.62755467  0.31377734  0.23863575\n",
      "   0.31377734  0.          0.          0.31377734  0.23863575  0.          0.\n",
      "   0.31377734  0.          0.          0.        ]\n",
      " [ 0.5         0.          0.          0.          0.          0.          0.\n",
      "   0.5         0.5         0.          0.          0.          0.5         0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.40301621  0.          0.          0.          0.30650422\n",
      "   0.          0.          0.          0.          0.30650422  0.40301621\n",
      "   0.          0.          0.40301621  0.40301621  0.40301621]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray()    \n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.          0.14628573]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(tfidf[0], tfidf).flatten()\n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理\n",
      "人事處議處洪智坤　柯P：不清楚議處結果\n",
      "留洪智坤 柯：殘障求職不易\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "for index in related_docs_indices:\n",
    "    print(ary[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree\n",
    "import jieba.analyse\n",
    "\n",
    "with open('D:\\\\OS DATA\\\\Documents\\\\1434435247.xml', 'r', encoding='utf-8') as f:\n",
    "    events=ElementTree.fromstring(f.read())\n",
    "\n",
    "corpus = []\n",
    "ary= []\n",
    "for elem in events.findall('./channel/item'):\n",
    "    title = elem.find('title').text\n",
    "    description = elem.find('description').text\n",
    "    ary.append(title)\n",
    "    corpus.append(' '.join(jieba.analyse.extract_tags(description, 20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "n_cosine_similarities = cosine_similarity(tfidf, tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_clusters=4)\n",
    "k_data = c.fit_predict(weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "with sqlite3.connect('C:\\\\Users\\\\User\\\\news2.sqlite') as db:\n",
    "    cur = db.cursor()\n",
    "    cur.execute('select title, summary, category from news_entry')\n",
    "    allNews = cur.fetchall()\n",
    "\n",
    "corpus = []\n",
    "tags = []\n",
    "for rec in allNews:\n",
    "    if (rec[2] == '娛樂') or (rec[2] == '社會'):\n",
    "        corpus.append(' '.join(jieba.cut(rec[1])))\n",
    "        tags.append(rec[2]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size=0.50, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(train_data,train_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95049504950495045"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(test_data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(test_tag, pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_tag, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
